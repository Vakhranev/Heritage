{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLMHvUjPEJNQ1w39/rfPl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/Heritage/blob/main/%D0%98%D1%81%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BF%D0%B0%D1%80%D1%81%D0%B5%D1%80%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T7SAAVp2l2dy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Функция для очистки текста от знаков препинания и приведения к нижнему регистру\n",
        "def clean_text(text):\n",
        "    # Приводим текст к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаляем знаки препинания, кроме дефисов внутри слов\n",
        "    return re.sub(r'(?<!\\w)-|[^\\w\\s-]', '', text)\n",
        "\n",
        "# Функция для подсчёта биграмм в тексте\n",
        "def count_bigrams(text):\n",
        "    tokens = text.split()\n",
        "    bigrams = zip(tokens, tokens[1:])\n",
        "    bigram_freq = Counter(bigrams)\n",
        "    return bigram_freq\n",
        "\n",
        "# Загрузка значений t_score из файла\n",
        "def load_t_scores(filename):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        print(f\"UnicodeDecodeError: {e}\")\n",
        "        print(\"Попробуем открыть файл в другой кодировке...\")\n",
        "        with open(filename, 'r', encoding='latin1') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "# Функция для проверки и корректировки биграммы с ё\n",
        "def check_yo_in_bigram(bigram, t_scores):\n",
        "    bigram_yo = bigram.replace('ё', 'е')\n",
        "    t_score = t_scores.get(bigram, 0)\n",
        "\n",
        "    # Если t_score == 0, проверяем биграмму с \"е\"\n",
        "    if t_score == 0 and bigram_yo != bigram:\n",
        "        t_score_yo = t_scores.get(bigram_yo, 0)\n",
        "        if t_score_yo != 0:\n",
        "            return t_score_yo  # Возвращаем t_score для биграммы с \"е\"\n",
        "\n",
        "    return t_score  # Возвращаем исходный t_score\n",
        "\n",
        "def process_america_texts(input_json, t_score_file, output_file):\n",
        "    # Загрузка текстов\n",
        "    with open(input_json, 'r', encoding='utf-8') as f:\n",
        "        america_texts = json.load(f)\n",
        "\n",
        "    # Загрузка t_scores\n",
        "    t_scores = load_t_scores(t_score_file)\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for key, data in america_texts.items():\n",
        "        # Исходный текст\n",
        "        text = data[\"text\"]\n",
        "\n",
        "        # Очистка текста\n",
        "        clean_text_content = clean_text(text)\n",
        "\n",
        "        # Токенизация текста\n",
        "        tokens = clean_text_content.split()\n",
        "\n",
        "        # Подсчёт частоты биграмм\n",
        "        bigram_freq = count_bigrams(clean_text_content)\n",
        "        bigram_freq = {\" \".join(bigram): freq for bigram, freq in bigram_freq.items()}\n",
        "\n",
        "        # Формирование биграмм с t_score\n",
        "        bigrams_unique = {}\n",
        "        for bigram, freq in bigram_freq.items():\n",
        "            t_score = check_yo_in_bigram(bigram, t_scores)  # Используем t_score с проверкой ё/е\n",
        "            bigrams_unique[bigram] = {\"t_score\": t_score}\n",
        "\n",
        "        # Сохранение результатов\n",
        "        result[key] = {\n",
        "            \"text\": text,\n",
        "            \"tokens\": tokens,\n",
        "            \"bigrams_freq\": bigram_freq,\n",
        "            \"bigrams_unique\": bigrams_unique,\n",
        "        }\n",
        "\n",
        "    # Запись в файл\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Пример использования\n",
        "input_json = \"america_texts.json\"\n",
        "t_score_file = \"cleaned_t_score_file.json\"\n",
        "output_file = \"updated_america_texts.json\"\n",
        "\n",
        "process_america_texts(input_json, t_score_file, output_file)"
      ]
    }
  ]
}